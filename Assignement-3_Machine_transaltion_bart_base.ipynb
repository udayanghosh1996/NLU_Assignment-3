{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce701ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BartForConditionalGeneration, AutoTokenizer, BartTokenizer\n",
    "import torch.optim\n",
    "import accelerate\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from random import sample\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "025e98c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_path = os.path.join(os.getcwd(), 'v3/v2/en-hi')\n",
    "model_path = os.path.join(os.path.join(os.getcwd(), 'Models'), 'Machine_Translation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "933d7c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data, max_length=512):\n",
    "        #self.data = self.load_data(data_path, sample_size)\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        #self.sample_size = sample_size\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_text = item['English']\n",
    "        target_text = item['Hindi']\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(input_text, max_length=self.max_length,\n",
    "                                            padding='max_length', truncation=True, return_tensors='pt'\n",
    "                                           )\n",
    "        \n",
    "        targets = self.tokenizer.encode(target_text, max_length=self.max_length,\n",
    "                                        padding='max_length', truncation=True, return_tensors='pt'\n",
    "                                        )\n",
    "        \n",
    "        return {'input_ids': inputs['input_ids'].squeeze(),\n",
    "                'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "                'labels': targets.squeeze()\n",
    "               }\n",
    "    \"\"\"def load_data(self, data_path, sample_size):\n",
    "        '''with open(data_path, 'r',encoding=\"utf8\") as file:\n",
    "            data = json.load(file)'''\n",
    "        data = []\n",
    "        with open(os.path.join(data_root_path, 'train.en'), 'r', encoding=\"utf8\") as en_file:\n",
    "            en_lines = en_file.readlines()\n",
    "        with open(os.path.join(data_root_path, 'train.hi'), 'r', encoding=\"utf8\") as hn_file:\n",
    "            hn_lines = hn_file.readlines()\n",
    "            \n",
    "        for en_line, hn_line in zip(en_lines, hn_lines):\n",
    "            data.append(dict(English=en_line.replace('\\n', ''), Hindi=hn_line.replace('\\n', '')))\n",
    "            \n",
    "        return sample(data, sample_size)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "181fbe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fab0df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path, sample_size):\n",
    "    '''with open(data_path, 'r',encoding=\"utf8\") as file:\n",
    "        data = json.load(file)'''\n",
    "    data = []\n",
    "    with open(os.path.join(data_root_path, 'train.en'), 'r', encoding=\"utf8\") as en_file:\n",
    "        en_lines = en_file.readlines()\n",
    "    with open(os.path.join(data_root_path, 'train.hi'), 'r', encoding=\"utf8\") as hn_file:\n",
    "        hn_lines = hn_file.readlines()\n",
    "\n",
    "    for en_line, hn_line in zip(en_lines, hn_lines):\n",
    "        data.append(dict(English=en_line.replace('\\n', ''), Hindi=hn_line.replace('\\n', '')))\n",
    "\n",
    "    return sample(data, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c9fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(data_root_path, 5600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "699e0a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.1785, shuffle=False)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.1304, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78504b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(tokenizer, train_data)\n",
    "dev_dataset = CustomDataset(tokenizer, val_data)\n",
    "test_dataset = CustomDataset(tokenizer, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72fe4215",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=25, shuffle=True)\n",
    "val_loader = DataLoader(dev_dataset, batch_size=25, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8b542a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fine_Tune(train_loader, val_loader, num_epochs, num_layers_to_finetune):\n",
    "    model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for i in range(1, num_layers_to_finetune + 1):\n",
    "        for param in model.model.encoder.layers[-i].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model.model.decoder.layers[-i].parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    #print(device)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=7e-5, weight_decay=3e-6)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        print(\"Training phase\")\n",
    "        #for idx, batch in enumerate(train_loader):\n",
    "        for batch in tqdm(train_loader):\n",
    "            #time0 = time()\n",
    "            #print(f\"batch: {idx+1} starts\")\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #time1 = time()\n",
    "            #print(f\"batch: {idx+1} ends\\ntime taken: {time1-time0} seconds\")\n",
    "            #print(f\"time taken: {time1-time0} seconds\")\n",
    "        \n",
    "        model.eval()\n",
    "        print(\"Validation phase\")\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0.0\n",
    "            for val_batch in tqdm(val_loader):\n",
    "                input_ids = val_batch['input_ids'].to(device)\n",
    "                attention_mask = val_batch['attention_mask'].to(device)\n",
    "                labels = val_batch['labels'].to(device)\n",
    "                val_output = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                total_val_loss += val_output.loss.item()\n",
    "            average_val_loss = total_val_loss / len(val_loader)\n",
    "            \n",
    "        print(f'Epoch: {epoch+1}/{num_epochs}, Validation Loss: {average_val_loss}')\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model.save_pretrained(model_path, from_pt=True)\n",
    "    torch.save(model.state_dict(), os.path.join(model_path, 'pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b51e3394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_translation(val_loader):\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_loader:\n",
    "            input_ids = val_batch['input_ids'].to(device)\n",
    "            attention_mask = val_batch['attention_mask'].to(device)\n",
    "            labels = val_batch['labels'].to(device)\n",
    "            generated_ids = model.generate(input_ids, attention_mask=attention_mask,\n",
    "                                           max_length=60, num_beams=2, repetition_penalty=2.0,\n",
    "                                           length_penalty=2.0, early_stopping=True\n",
    "                                          )\n",
    "            generated_title = tokenizer.decode(generated_ids[0])\n",
    "            input_text = tokenizer.decode(input_ids[0])\n",
    "            actual_title = tokenizer.decode(labels[0])\n",
    "            \n",
    "            print(f'Input Text: {input_text}\\nGenerated Title: {generated_title}\\nActual Title: {actual_title}')\n",
    "            print('\\n'+'='*50+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9254ec64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                                                                                   | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                                                                                                 | 0/160 [00:00<?, ?it/s]\u001b[AC:\\Users\\udaya\\miniconda3\\envs\\torch\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:590: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "\n",
      "  1%|█▍                                                                                                                                                                                                                                       | 1/160 [00:07<20:48,  7.85s/it]\u001b[A\n",
      "  1%|██▉                                                                                                                                                                                                                                      | 2/160 [00:08<09:11,  3.49s/it]\u001b[A\n",
      "  2%|████▎                                                                                                                                                                                                                                    | 3/160 [00:16<14:17,  5.46s/it]\u001b[A\n",
      "  2%|█████▊                                                                                                                                                                                                                                   | 4/160 [00:23<16:25,  6.32s/it]\u001b[A\n",
      "  3%|███████▎                                                                                                                                                                                                                                 | 5/160 [00:31<17:34,  6.81s/it]\u001b[A\n",
      "  4%|████████▋                                                                                                                                                                                                                                | 6/160 [00:39<18:11,  7.09s/it]\u001b[A\n",
      "  4%|██████████▏                                                                                                                                                                                                                              | 7/160 [00:46<18:33,  7.28s/it]\u001b[A\n",
      "  5%|███████████▋                                                                                                                                                                                                                             | 8/160 [00:54<18:41,  7.38s/it]\u001b[A\n",
      "  6%|█████████████                                                                                                                                                                                                                            | 9/160 [01:01<18:47,  7.47s/it]\u001b[A\n",
      "  6%|██████████████▌                                                                                                                                                                                                                         | 10/160 [01:09<18:45,  7.50s/it]\u001b[A\n",
      "  7%|███████████████▉                                                                                                                                                                                                                        | 11/160 [01:17<18:45,  7.56s/it]\u001b[A\n",
      "  8%|█████████████████▍                                                                                                                                                                                                                      | 12/160 [01:24<18:41,  7.58s/it]\u001b[A\n",
      "  8%|██████████████████▊                                                                                                                                                                                                                     | 13/160 [01:32<18:37,  7.60s/it]\u001b[A\n",
      "  9%|████████████████████▎                                                                                                                                                                                                                   | 14/160 [01:40<18:30,  7.60s/it]\u001b[A\n",
      "  9%|█████████████████████▊                                                                                                                                                                                                                  | 15/160 [01:47<18:24,  7.62s/it]\u001b[A\n",
      " 10%|███████████████████████▏                                                                                                                                                                                                                | 16/160 [01:55<18:16,  7.62s/it]\u001b[A\n",
      " 11%|████████████████████████▋                                                                                                                                                                                                               | 17/160 [02:03<18:10,  7.63s/it]\u001b[A\n",
      " 11%|██████████████████████████                                                                                                                                                                                                              | 18/160 [02:10<18:02,  7.62s/it]\u001b[A\n",
      " 12%|███████████████████████████▌                                                                                                                                                                                                            | 19/160 [02:18<17:56,  7.64s/it]\u001b[A\n",
      " 12%|█████████████████████████████                                                                                                                                                                                                           | 20/160 [02:26<17:52,  7.66s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "Fine_Tune(train_loader, val_loader, 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a57dbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Generate_translation(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e6d74d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
